{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangling Data for Predicting Hurricane Impacts\n",
    "* ### From the World Ocean Database for Ocean water data:\n",
    "    * Salinity, Temperature, Oxygen, Phosphates, and Silicates\n",
    "    \n",
    "* ### From the NOAA HURDAT Archive:\n",
    "    * Hurricanes, Categories, and Landfall Locations\n",
    "    \n",
    "*********\n",
    "\n",
    "# World Ocean Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wodpy in c:\\users\\sethh\\anaconda3\\lib\\site-packages\n",
      "Requirement already satisfied: pandas in c:\\users\\sethh\\anaconda3\\lib\\site-packages (from wodpy)\n",
      "Requirement already satisfied: numpy in c:\\users\\sethh\\anaconda3\\lib\\site-packages (from wodpy)\n",
      "Requirement already satisfied: python-dateutil>=2 in c:\\users\\sethh\\anaconda3\\lib\\site-packages (from pandas->wodpy)\n",
      "Requirement already satisfied: pytz>=2011k in c:\\users\\sethh\\anaconda3\\lib\\site-packages (from pandas->wodpy)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sethh\\anaconda3\\lib\\site-packages (from python-dateutil>=2->pandas->wodpy)\n",
      "Requirement already satisfied: wikipedia in c:\\users\\sethh\\anaconda3\\lib\\site-packages\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\sethh\\anaconda3\\lib\\site-packages (from wikipedia)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\users\\sethh\\anaconda3\\lib\\site-packages (from wikipedia)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\sethh\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in c:\\users\\sethh\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in c:\\users\\sethh\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sethh\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia)\n",
      "Requirement already satisfied: bs4 in c:\\users\\sethh\\anaconda3\\lib\\site-packages\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\sethh\\anaconda3\\lib\\site-packages (from bs4)\n",
      "Requirement already satisfied: requests in c:\\users\\sethh\\anaconda3\\lib\\site-packages\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\sethh\\anaconda3\\lib\\site-packages (from requests)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in c:\\users\\sethh\\anaconda3\\lib\\site-packages (from requests)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in c:\\users\\sethh\\anaconda3\\lib\\site-packages (from requests)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sethh\\anaconda3\\lib\\site-packages (from requests)\n"
     ]
    }
   ],
   "source": [
    "#### This script loads files from NOAA World Ocean Database into a dataframe.\n",
    "\n",
    "# Import, unzip, and open NOAA World Ocean Database files (Gulf of Mexico relevant files) \n",
    "# Then consolidate info into one dataframe\n",
    "\n",
    "# install wodpy and wikipedia\n",
    "import pip\n",
    "pip.main(['install','wodpy'])\n",
    "pip.main(['install','wikipedia'])\n",
    "\n",
    "# install requests and Beautiful soup\n",
    "pip.main(['install','bs4'])\n",
    "pip.main(['install','requests'])\n",
    "\n",
    "# import packages\n",
    "import wikipedia as wp\n",
    "import urllib\n",
    "import gzip\n",
    "import os\n",
    "from datetime import datetime\n",
    "from wodpy import wod\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# WOD file list for Gulf of Mexico\n",
    "url_list = ([\"C:/Users/sethh/OneDrive/Desktop/Springboard/Capstone Project 1/OSDO7208.gz\",\n",
    "         \"C:/Users/sethh/OneDrive/Desktop/Springboard/Capstone Project 1/OSDO7209.gz\"])\n",
    "file_paths = []\n",
    "# unzip files and open into a list of unzipped files\n",
    "for url in url_list:\n",
    "    \n",
    "    file_paths.append(url[:-2] + 'dat')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    with gzip.open(url, 'rb') as inF:\n",
    "        s = inF.read()\n",
    "\n",
    "    with open(url[:-2] + 'dat' , 'wb') as outF:\n",
    "        outF.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# WOD file list for Gulf of Mexico (OSD data only)\n",
    "url_list = ([\"http://data.nodc.noaa.gov/woa/WOD/GEOGRAPHIC/OSD/OBS/OSDO7208.gz\",\n",
    "         \"http://data.nodc.noaa.gov/woa/WOD/GEOGRAPHIC/OSD/OBS/OSDO7209.gz\"])\n",
    "\n",
    "# unzip files and open into a list of unzipped files\n",
    "file_paths = []\n",
    "for url in url_list:\n",
    "    file_path = 'C:/Users/sethh/OneDrive/Desktop/Springboard/Capstone Project 1/test/' + url.split('/')[-1][:-2] + 'dat' \n",
    "    file_paths.append(file_path)\n",
    "    \n",
    "    file = urllib.request.urlretrieve(url, url.split('/')[-1])\n",
    "    \n",
    "    with gzip.open(file[0], 'rb') as inF:\n",
    "        s = inF.read()\n",
    "        \n",
    "    with open(file_path , 'wb') as outF:\n",
    "        outF.write(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The files is an iterator containing profiles.  These profiles can be transformed into dataframes containing the data taken by depth as information in columns.  Time and location are stored as dataframe attributes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now create a list of dataframes from WodProfiles from the unzipped files.\n",
    "df_list = []\n",
    "for path in file_paths:\n",
    "    j = 1\n",
    "    with open(path) as file:\n",
    "        # check to see if current profile is the last in the file and loop through all profiles in file to append to df list\n",
    "        while j:\n",
    "            try:\n",
    "                df_list.append(wod.WodProfile(file).df())\n",
    "                \n",
    "                  \n",
    "            except:\n",
    "                j = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upon examination of the dataframes from the profiles some are empty and void of data.  I will remove these from the list of profile dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def profile_field_select(df):\n",
    "    \"\"\"Takes a profile and returns select parameters from the profile as a pandas Dataframe.\"\"\"\n",
    "    dic = {'oxygen':df.loc[0,'oxygen'],'pressure':df.loc[0,'pressure'],'pH':df.loc[0,'pH'],\n",
    "           'phosphate':df.loc[0,'phosphate'],'salinity':df.loc[0,'salinity'],'silicate':df.loc[0,'silicate'],\n",
    "           'temperature':df.loc[0,'temperature'],'latitude':df.latitude,'longitude':df.longitude,\n",
    "           'day':df.day,'month':df.month,'time':df.time,'year':df.year}\n",
    "    return pd.DataFrame(dic, index=[0])\n",
    "\n",
    "# next pull relevant information from each profile () and then add into a dataframe of aggregated relevant World Ocean Database data\n",
    "wod_df = pd.DataFrame(columns=['oxygen','pressure','pH','phosphate','salinity','silicate',\n",
    "                               'temperature','latitude','longitude','day','month','time','year'])\n",
    "\n",
    "for df in df_list:\n",
    "    if not df.empty: #filter out blank dataframes\n",
    "        wod_df = wod_df.append(profile_field_select(df), ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reorganize dataframe to have one column with a datetime object by day.\n",
    "# Then drop unnecessary time columns and sort by newly created date column\n",
    "wod_df['date'] = pd.to_datetime(wod_df['year'].astype(str) + '-' + wod_df['month'].astype(str) + '-' + wod_df['day'].astype(str))\n",
    "wod_df = wod_df.drop(['day','month','year','time'],axis=1).sort_values('date')\n",
    "\n",
    "# create a new series of a tuple of combined lat and lon data (lat,lon) and delete the lat and lon series from the dataframe.\n",
    "wod_df['location'] = list(zip(wod_df.latitude, wod_df.longitude))\n",
    "wod_df = wod_df.drop(axis=1,labels=['latitude','longitude'])\n",
    "# delete data predating 1960 and drop rows with all NaN values\n",
    "wod_df = wod_df.loc[wod_df['date'] > '1960-01-01'].dropna(how='all')\n",
    "wod_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After looking at the data grouped by date, I decided to round the locations to the nearest degree.  Also, historically accurate data wasn't really available until satellites were used to track them circa 1960.  Thus, I removed any rows with dates predating 1960. \n",
    "\n",
    "### The data will also be resampled as a monthly average for each set of data at each latitude and longitude.\n",
    "\n",
    "### Also, the data is pretty sparse for pH and pressure, so these columns will be dropped as well.\n",
    "\n",
    "### NaN values were handled by filling them with the average values per location.  Leftover NaN values were then filled with the overall time average values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# round lat and lon\n",
    "wod_df.location = [str((np.round(lat),np.round(lon))) for lat, lon in wod_df.location]\n",
    "# set index for dataframe as the date and location and sort and store in new dataframe\n",
    "wod_df_loc = wod_df.set_index(['location','date']).sort_index().dropna(how='all')\n",
    "# now group the date levels by month within the separate locations and drop pH and pressure data\n",
    "wod_df_loc = wod_df_loc.groupby(['location']+[pd.Grouper(freq='M',level=-1)]).mean()\n",
    "wod_df_loc = wod_df_loc.drop(axis=1,labels=['pH','pressure'])\n",
    "# now fill NaN values with the mean per location and fill left over NaNs with overall average over time\n",
    "wod_df_loc = wod_df_loc.fillna(wod_df_loc.groupby(level='location').mean())\n",
    "wod_df_loc = wod_df_loc.fillna(method='bfill')\n",
    "wod_df_loc.info()\n",
    "wod_df_loc\n",
    "# assuring that there are no duplicate locations\n",
    "assert wod_df_loc.groupby('location').count().index.value_counts().max() == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After looking at the timestamps per location, most locations are sparse in data points.  I will compile a dataframe by month only losing the location information and treat the gulf of mexico as one big body of water.  \n",
    "### NaN values will be back filled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# getting the number of datapoints per location\n",
    "location_counts = wod_df_loc.groupby('location').count().oxygen\n",
    "\n",
    "# generating new dataframe based on date alone and sampled Monthly with NaN values bfill\n",
    "wod_df_date = (wod_df.drop(axis=1,labels=['location']).set_index('date').\n",
    "                          resample('M').mean().fillna(method='bfill').drop(axis=1,labels=['pressure','pH']))\n",
    "wod_df_date.info()\n",
    "wod_df_date\n",
    "\n",
    "# plot histogram of location data point density\n",
    "_ = location_counts.plot(kind='hist',bins=list(range(0,150,5)),width=4,figsize=(12,6))\n",
    "_ = plt.xlabel('Data Points per Location')\n",
    "_ = plt.xticks(range(0,150,5))\n",
    "_ = plt.ylabel('No. of Locations with Data Points')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Exploratory data analysis:\n",
    "* #### Box plots for each value based from time organized data.\n",
    "* #### Line plots for each value based from time organized data.\n",
    "****\n",
    "### Upon initial visuals it seems the outliers could be real and will not be deleted from the data.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# descriptive summary statistics of data\n",
    "print(wod_df_date.describe())\n",
    "# boxplots and lineplots of the std deviation shifts from means\n",
    "_ = ((wod_df_date - wod_df_date.mean())/wod_df_date.std()).plot(kind='box', figsize=(9,6))\n",
    "_ = plt.ylabel('Standard Deviation Shifts From Mean')\n",
    "_ = plt.title('Time Grouped Box Plots')\n",
    "plt.show()\n",
    "_ = ((wod_df_date - wod_df_date.mean())/wod_df_date.std()).plot(figsize=(9,6))\n",
    "_ = plt.ylabel('Standard Deviation Shifts From Mean')\n",
    "_ = plt.title('Time Grouped Line Plots')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Later on, the data can be analyzed as by date, by location, or by both to get a wide variety of stories and pictures.\n",
    "\n",
    "*****\n",
    "\n",
    "# Historical Hurricane Data for the United States Gulf Coast\n",
    "### The data comes from the wiki table [List of United States hurricanes](https://en.wikipedia.org/wiki/List_of_United_States_hurricanes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 86 entries, 1960-09-10 to 2017-10-08\n",
      "Data columns (total 3 columns):\n",
      "name        86 non-null object\n",
      "category    86 non-null object\n",
      "state       86 non-null object\n",
      "dtypes: object(3)\n",
      "memory usage: 2.7+ KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sethh\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>category</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1960-09-10</th>\n",
       "      <td>Donna</td>\n",
       "      <td>4</td>\n",
       "      <td>Florida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1960-09-15</th>\n",
       "      <td>Ethel</td>\n",
       "      <td>1</td>\n",
       "      <td>Mississippi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961-09-11</th>\n",
       "      <td>Carla</td>\n",
       "      <td>4</td>\n",
       "      <td>Texas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963-09-17</th>\n",
       "      <td>Cindy</td>\n",
       "      <td>1</td>\n",
       "      <td>Texas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1964-08-27</th>\n",
       "      <td>Cleo</td>\n",
       "      <td>2</td>\n",
       "      <td>Florida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1964-09-10</th>\n",
       "      <td>Dora</td>\n",
       "      <td>2</td>\n",
       "      <td>Florida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1964-09-15</th>\n",
       "      <td>Ethel</td>\n",
       "      <td>1</td>\n",
       "      <td>Louisiana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1964-10-03</th>\n",
       "      <td>Hilda</td>\n",
       "      <td>3</td>\n",
       "      <td>Louisiana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1964-10-14</th>\n",
       "      <td>Isbell</td>\n",
       "      <td>2</td>\n",
       "      <td>Florida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1965-09-08</th>\n",
       "      <td>Betsy</td>\n",
       "      <td>3</td>\n",
       "      <td>Florida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1965-09-10</th>\n",
       "      <td>Betsy</td>\n",
       "      <td>3</td>\n",
       "      <td>Louisiana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1966-06-09</th>\n",
       "      <td>Alma</td>\n",
       "      <td>2</td>\n",
       "      <td>Florida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1966-10-08</th>\n",
       "      <td>Inez</td>\n",
       "      <td>1</td>\n",
       "      <td>Florida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967-09-20</th>\n",
       "      <td>Beulah</td>\n",
       "      <td>3</td>\n",
       "      <td>Texas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1968-10-19</th>\n",
       "      <td>Gladys</td>\n",
       "      <td>2</td>\n",
       "      <td>Florida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1969-08-17</th>\n",
       "      <td>Camille</td>\n",
       "      <td>5</td>\n",
       "      <td>Louisiana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1969-08-18</th>\n",
       "      <td>Camille</td>\n",
       "      <td>5</td>\n",
       "      <td>Mississippi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1969-08-18</th>\n",
       "      <td>Camille</td>\n",
       "      <td>1</td>\n",
       "      <td>Alabama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-08-03</th>\n",
       "      <td>Celia</td>\n",
       "      <td>3</td>\n",
       "      <td>Texas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1971-09-10</th>\n",
       "      <td>Fern</td>\n",
       "      <td>1</td>\n",
       "      <td>Texas</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               name category        state\n",
       "1960-09-10    Donna        4      Florida\n",
       "1960-09-15    Ethel        1  Mississippi\n",
       "1961-09-11    Carla        4        Texas\n",
       "1963-09-17    Cindy        1        Texas\n",
       "1964-08-27     Cleo        2      Florida\n",
       "1964-09-10     Dora        2      Florida\n",
       "1964-09-15    Ethel        1    Louisiana\n",
       "1964-10-03    Hilda        3    Louisiana\n",
       "1964-10-14   Isbell        2      Florida\n",
       "1965-09-08    Betsy        3      Florida\n",
       "1965-09-10    Betsy        3    Louisiana\n",
       "1966-06-09     Alma        2      Florida\n",
       "1966-10-08     Inez        1      Florida\n",
       "1967-09-20   Beulah        3        Texas\n",
       "1968-10-19   Gladys        2      Florida\n",
       "1969-08-17  Camille        5    Louisiana\n",
       "1969-08-18  Camille        5  Mississippi\n",
       "1969-08-18  Camille        1      Alabama\n",
       "1970-08-03    Celia        3        Texas\n",
       "1971-09-10     Fern        1        Texas"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get the html source\n",
    "html = wp.page(\"List_of_United_States_hurricanes\").html().encode(\"UTF-8\")\n",
    "#Use pandas built in html reader to bring tables in only for gulf coast states\n",
    "hurricane_list = [pd.read_html(html,header=0)[i] for i in [1,3,5,9,16]]\n",
    "state_names = ['Alabama','Florida','Louisiana','Mississippi','Texas']\n",
    "#last row of tables are footnotes so drop them\n",
    "hurricanes = [df.drop([len(df.index)-1]) for df in hurricane_list]\n",
    "#create blank combined dataframe\n",
    "hurricanes_df = pd.DataFrame(columns=['name','category','state'])\n",
    "\n",
    "for state, hurricane in zip(state_names,hurricanes):\n",
    "    #separate the tables and reconcatenate vertically instead of horizontally\n",
    "    a = hurricane.iloc[:,:4]\n",
    "    b = hurricane.iloc[:,5:9]\n",
    "    b.columns = [str(name).split('.')[0] for name in hurricane.columns.values][5:9]\n",
    "    hurricane = pd.concat([a,b], ignore_index=True)\n",
    "    \n",
    "    #remove blank rows from newly concatenated dataframe\n",
    "    hurricane = hurricane.dropna(how='all').reset_index(drop=True)\n",
    "\n",
    "\n",
    "    #the date is seperated into two columns so bring together and store as a datetime object\n",
    "    hurricane['date'] = [datetime.strptime(str(hurricane['Date of closest approach'][i]) + ' ' + \n",
    "                                                   str(hurricane['Year'][i])[:4],'%B %d %Y') \n",
    "                         for i in range(0,len(hurricane.index))] \n",
    "    \n",
    "    #drop the redundant date columns\n",
    "    hurricane = hurricane.drop(labels=['Date of closest approach','Year'],axis=1)\n",
    "\n",
    "    #remove the 'notes' from the category column and rename it to category\n",
    "    hurricane.columns = ['name','category','date']\n",
    "    hurricane.category = [str(cat)[0] for cat in hurricane.category]\n",
    "    \n",
    "    #find special cases in quotes for name and set to unnamed\n",
    "    for index, name in enumerate(hurricane.name):\n",
    "        if '\"' in name:\n",
    "            hurricane.name[index] = 'Unnamed'\n",
    "            \n",
    "            \n",
    "\n",
    "    #set date as index and sort by index\n",
    "    hurricane = hurricane.set_index('date').sort_index()\n",
    "    \n",
    "    #create a state label for the dataframe\n",
    "    hurricane['state'] = state\n",
    "    \n",
    "    #absorb dataframe into combined dataframe for historical hurricane dataframe\n",
    "    hurricanes_df = hurricanes_df.append(hurricane)\n",
    "\n",
    "#to keep the times consistent with the WOD database remove dates before 1960\n",
    "hurricanes_df = hurricanes_df.loc['1960-01-01':,:].sort_index()\n",
    "\n",
    "hurricanes_df.info()\n",
    "hurricanes_df.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
